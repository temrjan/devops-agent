# RAG SYSTEM GUIDE
## –î–ª—è Claude Code ‚Äî LlamaIndex

> **–¶–µ–ª—å:** –ï–¥–∏–Ω—ã–π —Å—Ç–∏–ª—å —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ RAG —Å–∏—Å—Ç–µ–º  
> **–†–µ—Ñ–µ—Ä–µ–Ω—Å:** LlamaIndex –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è (docs.llamaindex.ai)  
> **–í–µ—Ä—Å–∏—è:** LlamaIndex 0.10+, Python 3.11+

---

## üéØ –ö–õ–Æ–ß–ï–í–´–ï –ü–†–ò–ù–¶–ò–ü–´

```
–í–°–ï–ì–î–ê                              –ù–ò–ö–û–ì–î–ê
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚úì IngestionPipeline –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏    ‚úó –ü—Ä—è–º–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –±–µ–∑ pipeline
‚úì Chunk size 400-800 –¥–ª—è prose      ‚úó –û–≥—Ä–æ–º–Ω—ã–µ chunks (>1500 tokens)
‚úì Overlap 10-20% –æ—Ç chunk size      ‚úó –ù—É–ª–µ–≤–æ–π overlap
‚úì Metadata extraction               ‚úó Chunks –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
‚úì Hybrid search (vector + BM25)     ‚úó –¢–æ–ª—å–∫–æ vector search
‚úì Reranking –¥–ª—è top-k               ‚úó –ü—Ä—è–º–æ–π top-k –±–µ–∑ rerank
‚úì Evaluation (RAGAS)                ‚úó RAG –±–µ–∑ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞
‚úì Caching –¥–ª—è embeddings            ‚úó –ü–æ–≤—Ç–æ—Ä–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ
```

---

## üìÅ –°–¢–†–£–ö–¢–£–†–ê –ü–†–û–ï–ö–¢–ê

```
rag_system/
‚îú‚îÄ‚îÄ .env                          # API keys
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ README.md
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py                   # Entry point
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ config/                   # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ settings.py           # Pydantic Settings
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ ingestion/                # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ loaders.py            # Document loaders
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py           # Ingestion pipeline
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ transformations.py    # Custom transformations
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ indexing/                 # –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vector_store.py       # Vector store setup
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ index.py              # Index creation
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ retrieval/                # –ü–æ–∏—Å–∫
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ retrievers.py         # Retriever configs
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rerankers.py          # Reranking
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ generation/               # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ query_engine.py       # Query engines
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prompts.py            # Custom prompts
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ evaluation/               # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metrics.py            # RAGAS metrics
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ api/                      # API endpoints
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ routes.py
‚îÇ
‚îú‚îÄ‚îÄ data/                         # –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep
‚îÇ
‚îú‚îÄ‚îÄ storage/                      # Persisted indices
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep
‚îÇ
‚îî‚îÄ‚îÄ tests/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îî‚îÄ‚îÄ test_retrieval.py
```

---

## ‚öôÔ∏è –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø

### Settings

```python
# src/config/settings.py

from pydantic import SecretStr
from pydantic_settings import BaseSettings, SettingsConfigDict


class Settings(BaseSettings):
    """RAG system configuration."""
    
    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        extra="ignore",
    )
    
    # LLM
    openai_api_key: SecretStr
    llm_model: str = "gpt-4o-mini"
    llm_temperature: float = 0.1
    
    # Embeddings
    embedding_model: str = "text-embedding-3-small"
    embedding_dimensions: int = 1536
    
    # Chunking
    chunk_size: int = 512
    chunk_overlap: int = 50
    
    # Retrieval
    similarity_top_k: int = 10
    rerank_top_n: int = 5
    
    # Vector Store
    vector_store_type: str = "qdrant"  # qdrant, pinecone, chroma
    qdrant_url: str = "http://localhost:6333"
    qdrant_collection: str = "documents"
    
    # Paths
    data_dir: str = "./data"
    storage_dir: str = "./storage"


settings = Settings()
```

### LlamaIndex Settings (Global)

```python
# src/config/llm_settings.py

from llama_index.core import Settings as LlamaSettings
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

from src.config.settings import settings


def configure_llama_index() -> None:
    """Configure global LlamaIndex settings."""
    
    # LLM
    LlamaSettings.llm = OpenAI(
        model=settings.llm_model,
        temperature=settings.llm_temperature,
        api_key=settings.openai_api_key.get_secret_value(),
    )
    
    # Embeddings
    LlamaSettings.embed_model = OpenAIEmbedding(
        model=settings.embedding_model,
        api_key=settings.openai_api_key.get_secret_value(),
    )
    
    # Chunking defaults
    LlamaSettings.chunk_size = settings.chunk_size
    LlamaSettings.chunk_overlap = settings.chunk_overlap
```

---

## üì• –ó–ê–ì–†–£–ó–ö–ê –î–û–ö–£–ú–ï–ù–¢–û–í (INGESTION)

### Document Loaders

```python
# src/ingestion/loaders.py

from pathlib import Path
from typing import List

from llama_index.core import SimpleDirectoryReader, Document
from llama_index.readers.file import PDFReader, DocxReader
from llama_index.readers.web import SimpleWebPageReader


def load_from_directory(
    directory: str | Path,
    recursive: bool = True,
    required_exts: list[str] | None = None,
) -> list[Document]:
    """Load documents from a directory."""
    
    reader = SimpleDirectoryReader(
        input_dir=str(directory),
        recursive=recursive,
        required_exts=required_exts or [".pdf", ".docx", ".txt", ".md"],
        filename_as_id=True,
    )
    
    return reader.load_data(show_progress=True)


def load_from_urls(urls: list[str]) -> list[Document]:
    """Load documents from web URLs."""
    
    reader = SimpleWebPageReader(html_to_text=True)
    return reader.load_data(urls)


def load_pdf(file_path: str | Path) -> list[Document]:
    """Load a single PDF file."""
    
    reader = PDFReader()
    return reader.load_data(file=Path(file_path))


def load_with_metadata(
    directory: str | Path,
    metadata_fn: callable = None,
) -> list[Document]:
    """Load documents with custom metadata extraction."""
    
    def default_metadata_fn(file_path: str) -> dict:
        path = Path(file_path)
        return {
            "file_name": path.name,
            "file_type": path.suffix,
            "directory": str(path.parent),
        }
    
    reader = SimpleDirectoryReader(
        input_dir=str(directory),
        file_metadata=metadata_fn or default_metadata_fn,
    )
    
    return reader.load_data()
```

### Ingestion Pipeline

```python
# src/ingestion/pipeline.py

from llama_index.core import Document
from llama_index.core.ingestion import IngestionPipeline, IngestionCache
from llama_index.core.node_parser import (
    SentenceSplitter,
    SemanticSplitterNodeParser,
    HierarchicalNodeParser,
)
from llama_index.core.extractors import (
    TitleExtractor,
    SummaryExtractor,
    QuestionsAnsweredExtractor,
    KeywordExtractor,
)
from llama_index.embeddings.openai import OpenAIEmbedding

from src.config.settings import settings


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –ë–∞–∑–æ–≤—ã–π Pipeline (–¥–ª—è –Ω–∞—á–∞–ª–∞)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def create_basic_pipeline(
    vector_store=None,
    cache=None,
) -> IngestionPipeline:
    """Create a basic ingestion pipeline."""
    
    return IngestionPipeline(
        transformations=[
            # 1. Chunking
            SentenceSplitter(
                chunk_size=settings.chunk_size,
                chunk_overlap=settings.chunk_overlap,
            ),
            # 2. Embedding
            OpenAIEmbedding(
                model=settings.embedding_model,
                api_key=settings.openai_api_key.get_secret_value(),
            ),
        ],
        vector_store=vector_store,
        cache=cache,
    )


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Advanced Pipeline (—Å metadata extraction)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def create_advanced_pipeline(
    vector_store=None,
    cache=None,
) -> IngestionPipeline:
    """Create an advanced pipeline with metadata extraction."""
    
    embed_model = OpenAIEmbedding(
        model=settings.embedding_model,
        api_key=settings.openai_api_key.get_secret_value(),
    )
    
    return IngestionPipeline(
        transformations=[
            # 1. Chunking
            SentenceSplitter(
                chunk_size=settings.chunk_size,
                chunk_overlap=settings.chunk_overlap,
            ),
            # 2. Metadata Extraction (–û–ü–¶–ò–û–ù–ê–õ–¨–ù–û ‚Äî –¥–æ—Ä–æ–≥–æ!)
            TitleExtractor(nodes=5),
            KeywordExtractor(keywords=5),
            # 3. Embedding
            embed_model,
        ],
        vector_store=vector_store,
        cache=cache,
    )


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Semantic Chunking Pipeline
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def create_semantic_pipeline(
    vector_store=None,
) -> IngestionPipeline:
    """Create a pipeline with semantic chunking."""
    
    embed_model = OpenAIEmbedding(
        model=settings.embedding_model,
        api_key=settings.openai_api_key.get_secret_value(),
    )
    
    return IngestionPipeline(
        transformations=[
            # Semantic splitter –≥—Ä—É–ø–ø–∏—Ä—É–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            SemanticSplitterNodeParser(
                buffer_size=1,
                breakpoint_percentile_threshold=95,
                embed_model=embed_model,
            ),
            embed_model,
        ],
        vector_store=vector_store,
    )


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Hierarchical Pipeline (Parent-Child)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def create_hierarchical_pipeline() -> IngestionPipeline:
    """Create a hierarchical chunking pipeline."""
    
    return IngestionPipeline(
        transformations=[
            HierarchicalNodeParser.from_defaults(
                chunk_sizes=[2048, 512, 128],  # parent -> child -> leaf
            ),
            OpenAIEmbedding(
                model=settings.embedding_model,
                api_key=settings.openai_api_key.get_secret_value(),
            ),
        ],
    )


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –ó–∞–ø—É—Å–∫ Pipeline
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def run_ingestion(
    documents: list[Document],
    pipeline: IngestionPipeline,
    show_progress: bool = True,
):
    """Run ingestion pipeline on documents."""
    
    nodes = pipeline.run(
        documents=documents,
        show_progress=show_progress,
    )
    
    return nodes
```

### Custom Transformations

```python
# src/ingestion/transformations.py

from typing import List, Sequence
from llama_index.core.schema import BaseNode, TransformComponent


class TextCleaner(TransformComponent):
    """Clean and normalize text in nodes."""
    
    def __call__(
        self,
        nodes: Sequence[BaseNode],
        **kwargs,
    ) -> List[BaseNode]:
        for node in nodes:
            # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
            text = node.get_content()
            text = " ".join(text.split())
            
            # –£–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            # text = text.replace("\x00", "")
            
            node.set_content(text)
        
        return list(nodes)


class MetadataEnricher(TransformComponent):
    """Add custom metadata to nodes."""
    
    def __init__(self, source_name: str):
        self.source_name = source_name
    
    def __call__(
        self,
        nodes: Sequence[BaseNode],
        **kwargs,
    ) -> List[BaseNode]:
        for node in nodes:
            node.metadata["source"] = self.source_name
            node.metadata["char_count"] = len(node.get_content())
        
        return list(nodes)
```

---

## üóÑÔ∏è VECTOR STORES

### Qdrant Setup

```python
# src/indexing/vector_store.py

from qdrant_client import QdrantClient
from llama_index.vector_stores.qdrant import QdrantVectorStore
from llama_index.vector_stores.chroma import ChromaVectorStore
import chromadb

from src.config.settings import settings


def get_qdrant_vector_store(
    collection_name: str | None = None,
    url: str | None = None,
) -> QdrantVectorStore:
    """Get Qdrant vector store."""
    
    client = QdrantClient(
        url=url or settings.qdrant_url,
    )
    
    return QdrantVectorStore(
        client=client,
        collection_name=collection_name or settings.qdrant_collection,
    )


def get_qdrant_memory() -> QdrantVectorStore:
    """Get in-memory Qdrant for development."""
    
    client = QdrantClient(location=":memory:")
    
    return QdrantVectorStore(
        client=client,
        collection_name="dev_collection",
    )


def get_chroma_vector_store(
    collection_name: str = "documents",
    persist_dir: str = "./chroma_db",
) -> ChromaVectorStore:
    """Get ChromaDB vector store."""
    
    client = chromadb.PersistentClient(path=persist_dir)
    collection = client.get_or_create_collection(collection_name)
    
    return ChromaVectorStore(chroma_collection=collection)
```

### Index Creation

```python
# src/indexing/index.py

from pathlib import Path

from llama_index.core import (
    VectorStoreIndex,
    StorageContext,
    load_index_from_storage,
)
from llama_index.core.schema import BaseNode

from src.config.settings import settings
from src.indexing.vector_store import get_qdrant_vector_store


def create_index_from_nodes(
    nodes: list[BaseNode],
    vector_store=None,
) -> VectorStoreIndex:
    """Create index from pre-processed nodes."""
    
    if vector_store:
        storage_context = StorageContext.from_defaults(
            vector_store=vector_store,
        )
        index = VectorStoreIndex(
            nodes=nodes,
            storage_context=storage_context,
            show_progress=True,
        )
    else:
        index = VectorStoreIndex(
            nodes=nodes,
            show_progress=True,
        )
    
    return index


def create_index_from_vector_store(
    vector_store=None,
) -> VectorStoreIndex:
    """Create index from existing vector store."""
    
    vector_store = vector_store or get_qdrant_vector_store()
    
    return VectorStoreIndex.from_vector_store(
        vector_store=vector_store,
    )


def persist_index(
    index: VectorStoreIndex,
    persist_dir: str | None = None,
) -> None:
    """Persist index to disk."""
    
    persist_dir = persist_dir or settings.storage_dir
    index.storage_context.persist(persist_dir=persist_dir)


def load_index(
    persist_dir: str | None = None,
) -> VectorStoreIndex:
    """Load index from disk."""
    
    persist_dir = persist_dir or settings.storage_dir
    
    if not Path(persist_dir).exists():
        raise FileNotFoundError(f"Index not found at {persist_dir}")
    
    storage_context = StorageContext.from_defaults(persist_dir=persist_dir)
    return load_index_from_storage(storage_context)
```

---

## üîç RETRIEVAL

### Retrievers

```python
# src/retrieval/retrievers.py

from llama_index.core import VectorStoreIndex
from llama_index.core.retrievers import (
    VectorIndexRetriever,
    RouterRetriever,
)
from llama_index.retrievers.bm25 import BM25Retriever
from llama_index.core.schema import NodeWithScore

from src.config.settings import settings


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Basic Vector Retriever
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def get_vector_retriever(
    index: VectorStoreIndex,
    similarity_top_k: int | None = None,
) -> VectorIndexRetriever:
    """Get basic vector retriever."""
    
    return VectorIndexRetriever(
        index=index,
        similarity_top_k=similarity_top_k or settings.similarity_top_k,
    )


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Hybrid Retriever (Vector + BM25)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class HybridRetriever:
    """Combine vector and BM25 retrieval."""
    
    def __init__(
        self,
        index: VectorStoreIndex,
        nodes: list,
        vector_weight: float = 0.7,
        bm25_weight: float = 0.3,
        top_k: int = 10,
    ):
        self.vector_retriever = VectorIndexRetriever(
            index=index,
            similarity_top_k=top_k,
        )
        self.bm25_retriever = BM25Retriever.from_defaults(
            nodes=nodes,
            similarity_top_k=top_k,
        )
        self.vector_weight = vector_weight
        self.bm25_weight = bm25_weight
        self.top_k = top_k
    
    def retrieve(self, query: str) -> list[NodeWithScore]:
        """Retrieve using both methods and combine scores."""
        
        # Get results from both retrievers
        vector_results = self.vector_retriever.retrieve(query)
        bm25_results = self.bm25_retriever.retrieve(query)
        
        # Combine and normalize scores
        node_scores: dict[str, float] = {}
        node_map: dict[str, NodeWithScore] = {}
        
        # Add vector scores
        for node in vector_results:
            node_id = node.node.node_id
            node_scores[node_id] = node.score * self.vector_weight
            node_map[node_id] = node
        
        # Add BM25 scores
        for node in bm25_results:
            node_id = node.node.node_id
            if node_id in node_scores:
                node_scores[node_id] += node.score * self.bm25_weight
            else:
                node_scores[node_id] = node.score * self.bm25_weight
                node_map[node_id] = node
        
        # Sort by combined score
        sorted_nodes = sorted(
            node_scores.items(),
            key=lambda x: x[1],
            reverse=True,
        )[:self.top_k]
        
        # Return NodeWithScore objects
        results = []
        for node_id, score in sorted_nodes:
            node_with_score = node_map[node_id]
            node_with_score.score = score
            results.append(node_with_score)
        
        return results


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Auto-Merging Retriever (Hierarchical)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

from llama_index.core.retrievers import AutoMergingRetriever
from llama_index.core.storage.docstore import SimpleDocumentStore


def get_auto_merging_retriever(
    index: VectorStoreIndex,
    docstore: SimpleDocumentStore,
    top_k: int = 6,
) -> AutoMergingRetriever:
    """Get auto-merging retriever for hierarchical nodes."""
    
    base_retriever = index.as_retriever(similarity_top_k=top_k)
    
    return AutoMergingRetriever(
        base_retriever,
        storage_context=index.storage_context,
        simple_ratio_thresh=0.5,  # Merge if >50% of children are retrieved
    )
```

### Reranking

```python
# src/retrieval/rerankers.py

from llama_index.core.postprocessor import (
    SentenceTransformerRerank,
    LLMRerank,
    SimilarityPostprocessor,
)
from llama_index.postprocessor.cohere_rerank import CohereRerank

from src.config.settings import settings


def get_sentence_transformer_reranker(
    top_n: int | None = None,
    model: str = "cross-encoder/ms-marco-MiniLM-L-6-v2",
):
    """Get sentence transformer reranker (local, fast)."""
    
    return SentenceTransformerRerank(
        model=model,
        top_n=top_n or settings.rerank_top_n,
    )


def get_cohere_reranker(
    api_key: str,
    top_n: int | None = None,
    model: str = "rerank-english-v3.0",
):
    """Get Cohere reranker (API, high quality)."""
    
    return CohereRerank(
        api_key=api_key,
        model=model,
        top_n=top_n or settings.rerank_top_n,
    )


def get_llm_reranker(
    top_n: int | None = None,
    choice_batch_size: int = 5,
):
    """Get LLM-based reranker (expensive but flexible)."""
    
    return LLMRerank(
        top_n=top_n or settings.rerank_top_n,
        choice_batch_size=choice_batch_size,
    )


def get_similarity_filter(
    similarity_cutoff: float = 0.7,
):
    """Filter nodes by similarity threshold."""
    
    return SimilarityPostprocessor(
        similarity_cutoff=similarity_cutoff,
    )
```

---

## üí¨ QUERY ENGINES

### Basic Query Engine

```python
# src/generation/query_engine.py

from llama_index.core import VectorStoreIndex
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.response_synthesizers import (
    ResponseMode,
    get_response_synthesizer,
)

from src.retrieval.retrievers import get_vector_retriever
from src.retrieval.rerankers import get_sentence_transformer_reranker
from src.config.settings import settings


def create_basic_query_engine(
    index: VectorStoreIndex,
    similarity_top_k: int | None = None,
):
    """Create a basic query engine."""
    
    return index.as_query_engine(
        similarity_top_k=similarity_top_k or settings.similarity_top_k,
    )


def create_reranking_query_engine(
    index: VectorStoreIndex,
    similarity_top_k: int | None = None,
    rerank_top_n: int | None = None,
):
    """Create query engine with reranking."""
    
    retriever = get_vector_retriever(
        index=index,
        similarity_top_k=similarity_top_k or settings.similarity_top_k,
    )
    
    reranker = get_sentence_transformer_reranker(
        top_n=rerank_top_n or settings.rerank_top_n,
    )
    
    response_synthesizer = get_response_synthesizer(
        response_mode=ResponseMode.COMPACT,
    )
    
    return RetrieverQueryEngine(
        retriever=retriever,
        node_postprocessors=[reranker],
        response_synthesizer=response_synthesizer,
    )


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Response Modes
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def create_query_engine_with_mode(
    index: VectorStoreIndex,
    response_mode: str = "compact",
):
    """
    Create query engine with specific response mode.
    
    Response modes:
    - "refine": Iterate through each node, refining answer
    - "compact": Stuff as much context as possible, then refine
    - "tree_summarize": Recursively summarize chunks
    - "simple_summarize": Simple concatenation and summarize
    - "accumulate": Synthesize answer for each node, then combine
    - "compact_accumulate": Compact + accumulate
    """
    
    mode_map = {
        "refine": ResponseMode.REFINE,
        "compact": ResponseMode.COMPACT,
        "tree_summarize": ResponseMode.TREE_SUMMARIZE,
        "simple_summarize": ResponseMode.SIMPLE_SUMMARIZE,
        "accumulate": ResponseMode.ACCUMULATE,
        "compact_accumulate": ResponseMode.COMPACT_ACCUMULATE,
    }
    
    response_synthesizer = get_response_synthesizer(
        response_mode=mode_map.get(response_mode, ResponseMode.COMPACT),
    )
    
    return index.as_query_engine(
        response_synthesizer=response_synthesizer,
    )
```

### Custom Prompts

```python
# src/generation/prompts.py

from llama_index.core import PromptTemplate
from llama_index.core.prompts import PromptType


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# QA Prompt (–¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

QA_PROMPT_TMPL = """\
–ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∏–∂–µ:
---------------------
{context_str}
---------------------

–ò—Å–ø–æ–ª—å–∑—É—è –¢–û–õ–¨–ö–û –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π –≤–Ω–µ—à–Ω–∏–µ –∑–Ω–∞–Ω–∏—è), \
–æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å.

–ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–∞–π–¥–µ–Ω –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, —Å–∫–∞–∂–∏: \
"–Ø –Ω–µ –Ω–∞—à—ë–ª –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö."

–í–æ–ø—Ä–æ—Å: {query_str}

–û—Ç–≤–µ—Ç: """

QA_PROMPT = PromptTemplate(
    template=QA_PROMPT_TMPL,
    prompt_type=PromptType.QUESTION_ANSWER,
)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Refine Prompt (–¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

REFINE_PROMPT_TMPL = """\
–ò—Å—Ö–æ–¥–Ω—ã–π –≤–æ–ø—Ä–æ—Å: {query_str}

–¢–µ–∫—É—â–∏–π –æ—Ç–≤–µ—Ç: {existing_answer}

–£ –Ω–∞—Å –µ—Å—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —É—Ç–æ—á–Ω–∏—Ç—å —Ç–µ–∫—É—â–∏–π –æ—Ç–≤–µ—Ç (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ) \
–∏—Å–ø–æ–ª—å–∑—É—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∏–∂–µ.
------------
{context_msg}
------------

–ò—Å–ø–æ–ª—å–∑—É—è –Ω–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, —É—Ç–æ—á–Ω–∏ –∏—Å—Ö–æ–¥–Ω—ã–π –æ—Ç–≤–µ—Ç, —á—Ç–æ–±—ã –æ–Ω –ª—É—á—à–µ \
–æ—Ç–≤–µ—á–∞–ª –Ω–∞ –≤–æ–ø—Ä–æ—Å. –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –ø–æ–ª–µ–∑–µ–Ω, –≤–µ—Ä–Ω–∏ –∏—Å—Ö–æ–¥–Ω—ã–π –æ—Ç–≤–µ—Ç.

–£—Ç–æ—á–Ω—ë–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç: """

REFINE_PROMPT = PromptTemplate(
    template=REFINE_PROMPT_TMPL,
    prompt_type=PromptType.REFINE,
)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def create_query_engine_with_prompts(
    index,
    qa_prompt: PromptTemplate = QA_PROMPT,
    refine_prompt: PromptTemplate = REFINE_PROMPT,
):
    """Create query engine with custom prompts."""
    
    query_engine = index.as_query_engine()
    
    query_engine.update_prompts({
        "response_synthesizer:text_qa_template": qa_prompt,
        "response_synthesizer:refine_template": refine_prompt,
    })
    
    return query_engine
```

---

## üìä EVALUATION (RAGAS)

```python
# src/evaluation/metrics.py

from typing import List
from dataclasses import dataclass

from llama_index.core import VectorStoreIndex
from llama_index.core.evaluation import (
    FaithfulnessEvaluator,
    RelevancyEvaluator,
    CorrectnessEvaluator,
    BatchEvalRunner,
)


@dataclass
class EvalResult:
    """Evaluation result container."""
    faithfulness: float
    relevancy: float
    correctness: float | None = None


def evaluate_single_response(
    query: str,
    response: str,
    contexts: list[str],
    reference: str | None = None,
) -> EvalResult:
    """Evaluate a single RAG response."""
    
    faithfulness_evaluator = FaithfulnessEvaluator()
    relevancy_evaluator = RelevancyEvaluator()
    
    # Faithfulness: Is the response grounded in the context?
    faithfulness_result = faithfulness_evaluator.evaluate(
        query=query,
        response=response,
        contexts=contexts,
    )
    
    # Relevancy: Is the response relevant to the query?
    relevancy_result = relevancy_evaluator.evaluate(
        query=query,
        response=response,
    )
    
    result = EvalResult(
        faithfulness=faithfulness_result.score,
        relevancy=relevancy_result.score,
    )
    
    # Correctness: Is the response correct? (requires reference)
    if reference:
        correctness_evaluator = CorrectnessEvaluator()
        correctness_result = correctness_evaluator.evaluate(
            query=query,
            response=response,
            reference=reference,
        )
        result.correctness = correctness_result.score
    
    return result


async def evaluate_batch(
    queries: list[str],
    query_engine,
    references: list[str] | None = None,
) -> dict:
    """Batch evaluate multiple queries."""
    
    evaluators = {
        "faithfulness": FaithfulnessEvaluator(),
        "relevancy": RelevancyEvaluator(),
    }
    
    if references:
        evaluators["correctness"] = CorrectnessEvaluator()
    
    runner = BatchEvalRunner(
        evaluators=evaluators,
        workers=4,
    )
    
    results = await runner.aevaluate_queries(
        query_engine=query_engine,
        queries=queries,
    )
    
    return results


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# RAGAS Integration (–µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω ragas)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def evaluate_with_ragas(
    questions: list[str],
    answers: list[str],
    contexts: list[list[str]],
    ground_truths: list[str] | None = None,
):
    """
    Evaluate using RAGAS metrics.
    
    pip install ragas
    """
    try:
        from ragas import evaluate
        from ragas.metrics import (
            faithfulness,
            answer_relevancy,
            context_precision,
            context_recall,
        )
        from datasets import Dataset
    except ImportError:
        raise ImportError("Install ragas: pip install ragas datasets")
    
    data = {
        "question": questions,
        "answer": answers,
        "contexts": contexts,
    }
    
    if ground_truths:
        data["ground_truth"] = ground_truths
    
    dataset = Dataset.from_dict(data)
    
    metrics = [faithfulness, answer_relevancy, context_precision]
    if ground_truths:
        metrics.append(context_recall)
    
    result = evaluate(dataset, metrics=metrics)
    
    return result
```

---

## üöÄ –ü–û–õ–ù–´–ô –ü–†–ò–ú–ï–†

```python
# src/main.py

import asyncio
from pathlib import Path

from src.config.settings import settings
from src.config.llm_settings import configure_llama_index
from src.ingestion.loaders import load_from_directory
from src.ingestion.pipeline import create_basic_pipeline, run_ingestion
from src.indexing.vector_store import get_qdrant_memory
from src.indexing.index import create_index_from_nodes
from src.generation.query_engine import create_reranking_query_engine


async def main():
    # 1. Configure LlamaIndex
    configure_llama_index()
    
    # 2. Load documents
    print("üì• Loading documents...")
    documents = load_from_directory(settings.data_dir)
    print(f"   Loaded {len(documents)} documents")
    
    # 3. Create vector store
    vector_store = get_qdrant_memory()  # In-memory for dev
    
    # 4. Create and run ingestion pipeline
    print("‚öôÔ∏è Running ingestion pipeline...")
    pipeline = create_basic_pipeline(vector_store=vector_store)
    nodes = run_ingestion(documents, pipeline)
    print(f"   Created {len(nodes)} nodes")
    
    # 5. Create index
    print("üìä Creating index...")
    index = create_index_from_nodes(nodes, vector_store=vector_store)
    
    # 6. Create query engine
    query_engine = create_reranking_query_engine(index)
    
    # 7. Query!
    print("\nüîç Ready for queries!\n")
    
    while True:
        query = input("Query (or 'quit'): ").strip()
        if query.lower() in ("quit", "exit", "q"):
            break
        
        response = query_engine.query(query)
        
        print(f"\nüìù Answer: {response.response}\n")
        print(f"üìö Sources: {len(response.source_nodes)} nodes used\n")


if __name__ == "__main__":
    asyncio.run(main())
```

---

## ‚úÖ –ß–ï–ö–õ–ò–°–¢

```
INGESTION
‚ñ° IngestionPipeline –Ω–∞—Å—Ç—Ä–æ–µ–Ω
‚ñ° Chunk size 400-800 tokens
‚ñ° Overlap 10-20%
‚ñ° Metadata extraction (title, keywords)
‚ñ° Caching –¥–ª—è embeddings

RETRIEVAL
‚ñ° Similarity top_k = 10-20
‚ñ° Hybrid search (vector + BM25)
‚ñ° Reranking (Cohere –∏–ª–∏ sentence-transformers)
‚ñ° Rerank top_n = 3-5

GENERATION
‚ñ° Custom prompts –Ω–∞ –Ω—É–∂–Ω–æ–º —è–∑—ã–∫–µ
‚ñ° Response mode = compact –∏–ª–∏ refine
‚ñ° Streaming –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤

EVALUATION
‚ñ° Faithfulness > 0.8
‚ñ° Relevancy > 0.8
‚ñ° Context precision tracked
‚ñ° A/B —Ç–µ—Å—Ç—ã –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö

PRODUCTION
‚ñ° Persistent vector store (Qdrant, Pinecone)
‚ñ° Redis cache –¥–ª—è embeddings
‚ñ° Async –≥–¥–µ –≤–æ–∑–º–æ–∂–Ω–æ
‚ñ° Error handling
‚ñ° Logging
```

---

## üöÄ –ë–´–°–¢–†–´–ô –ü–†–û–ú–ü–¢ –î–õ–Ø CLAUDE CODE

```
RAG —Å–∏—Å—Ç–µ–º–∞ –Ω–∞ LlamaIndex. –°–ª–µ–¥—É–π docs.llamaindex.ai:

INGESTION:
- IngestionPipeline —Å transformations
- SentenceSplitter(chunk_size=512, chunk_overlap=50)
- OpenAIEmbedding –≤ –∫–æ–Ω—Ü–µ pipeline
- TitleExtractor, KeywordExtractor –¥–ª—è metadata

VECTOR STORE:
- Qdrant –∏–ª–∏ ChromaDB –¥–ª—è dev
- Pinecone –¥–ª—è production
- VectorStoreIndex.from_vector_store()

RETRIEVAL:
- VectorIndexRetriever(similarity_top_k=10)
- BM25Retriever –¥–ª—è hybrid search
- SentenceTransformerRerank(top_n=5)

QUERY ENGINE:
- RetrieverQueryEngine —Å node_postprocessors
- ResponseMode.COMPACT
- Custom PromptTemplate –¥–ª—è QA

–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û:
‚úÖ Settings —á–µ—Ä–µ–∑ Pydantic
‚úÖ Persist/load index
‚úÖ Evaluation metrics (faithfulness, relevancy)

CHUNK SIZE GUIDE:
- Prose/–¥–æ–∫—É–º–µ–Ω—Ç—ã: 400-800 tokens
- –ö–æ–¥: 80-160 tokens
- Overlap: 10-20% –æ—Ç chunk_size
```

---

**–í–µ—Ä—Å–∏—è:** 1.0  
**–î–∞—Ç–∞:** 01.12.2025  
**–†–µ—Ñ–µ—Ä–µ–Ω—Å:** LlamaIndex documentation (docs.llamaindex.ai)
